{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b82ca6",
   "metadata": {},
   "source": [
    "# ðŸ§  NLP with Pre-trained Models\n",
    "\n",
    "This notebook demonstrates how to use pre-trained models for common NLP tasks using the Hugging Face `transformers` library and related tools.\n",
    "\n",
    "Don't forget to do `poetry add transformers torch peft`\n",
    "\n",
    "To use notebooks with poetry may also need to install jupyter kernel:\n",
    "```\n",
    "poetry add jupyter ipykernel\n",
    "poetry run python -m ipykernel install --name youarebot-quickstart --user --display-name youarebot-quickstart\n",
    "```\n",
    "\n",
    "install Jupyter VSCode plugins and reload VSCode (maybe)\n",
    "\n",
    "NOTE: running todays notebooks will download many big models (~1Gb or more) to your laptop, so be careful when running this if you're low on free disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18a495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d12be",
   "metadata": {},
   "source": [
    "## QQ: which NLP tasks do you know?\n",
    "Some examples:\n",
    "- Classification (sentiment and topic)\n",
    "- NER (entity extraction)\n",
    "- Translation\n",
    "- Summarization\n",
    "- Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60c3a5",
   "metadata": {},
   "source": [
    "## 2. Text Classification Demo\n",
    "\n",
    "Let's classify the sentiment of some example sentences using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230f07be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love using pre-trained models!\n",
      "Result: [{'label': 'POSITIVE', 'score': 0.9993071556091309}]\n",
      "\n",
      "Text: This product is terrible and I want a refund.\n",
      "Result: [{'label': 'NEGATIVE', 'score': 0.9997045397758484}]\n",
      "\n",
      "Text: The movie was okay, not great but not bad either.\n",
      "Result: [{'label': 'POSITIVE', 'score': 0.9919201135635376}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment classification examples\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "texts = [\n",
    "    \"I love using pre-trained models!\",\n",
    "    \"This product is terrible and I want a refund.\",\n",
    "    \"The movie was okay, not great but not bad either.\"\n",
    "]\n",
    "for text in texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Text: {text}\\nResult: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173f0b1",
   "metadata": {},
   "source": [
    "## 3. Named Entity Recognition (NER) Demo\n",
    "\n",
    "Extract named entities from text using a pre-trained NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24bf10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple is looking at buying U.K. startup for $1 billion.\n",
      "Entities: [{'entity_group': 'ORG', 'score': np.float32(0.9990897), 'word': 'Apple', 'start': 0, 'end': 5}, {'entity_group': 'LOC', 'score': np.float32(0.999718), 'word': 'U', 'start': 27, 'end': 28}, {'entity_group': 'LOC', 'score': np.float32(0.9987226), 'word': 'K', 'start': 29, 'end': 30}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aguschin/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/pipelines/token_classification.py:181: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# NER example\n",
    "from transformers import pipeline\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "entities = ner(text)\n",
    "print(f\"Text: {text}\\nEntities: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be6e95",
   "metadata": {},
   "source": [
    "## 4. Text Translation Demo\n",
    "\n",
    "Translate English text to French using a pre-trained translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649b0779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Machine learning is amazing.\n",
      "French: L'apprentissage par machine est Ã©tonnant.\n"
     ]
    }
   ],
   "source": [
    "# Translation example\n",
    "from transformers import pipeline\n",
    "translator = pipeline('translation_en_to_fr')\n",
    "text = \"Machine learning is amazing.\"\n",
    "translation = translator(text)\n",
    "print(f\"Original: {text}\\nFrench: {translation[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a1744",
   "metadata": {},
   "source": [
    "## 5. Summarization Demo\n",
    "\n",
    "Summarize a long text using a pre-trained summarization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cca76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to learn from data without being explicitly programmed . Machine learning algorithms build a model based on sample data\n"
     ]
    }
   ],
   "source": [
    "# Summarization example\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline('summarization')\n",
    "long_text = (\n",
    "    \"Machine learning is a field of artificial intelligence that uses statistical techniques \"\n",
    "    \"to give computer systems the ability to learn from data, without being explicitly programmed. \"\n",
    "    \"It is seen as a part of artificial intelligence. Machine learning algorithms build a model \"\n",
    "    \"based on sample data, known as training data, in order to make predictions or decisions \"\n",
    "    \"without being explicitly programmed to do so.\"\n",
    ")\n",
    "summary = summarizer(long_text, max_length=40, min_length=10, do_sample=False)\n",
    "print(f\"Summary: {summary[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658e855",
   "metadata": {},
   "source": [
    "## 6. Zero-shot Classification Demo\n",
    "\n",
    "Classify text into arbitrary categories using a zero-shot model like `facebook/bart-large-mnli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e9a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is a fantastic new phone with a great camera.\n",
      "Labels: ['technology', 'sports', 'politics', 'entertainment']\n",
      "Result: {'sequence': 'This is a fantastic new phone with a great camera.', 'labels': ['technology', 'entertainment', 'sports', 'politics'], 'scores': [0.8937835693359375, 0.09825354814529419, 0.006223480682820082, 0.0017393830930814147]}\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot classification example\n",
    "from transformers import pipeline\n",
    "zero_shot = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
    "text = \"This is a fantastic new phone with a great camera.\"\n",
    "labels = [\"technology\", \"sports\", \"politics\", \"entertainment\"]\n",
    "result = zero_shot(text, candidate_labels=labels)\n",
    "print(f\"Text: {text}\\nLabels: {labels}\\nResult: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9ce12",
   "metadata": {},
   "source": [
    "## 7. Text Generation Demo\n",
    "\n",
    "Generate text using a sequence-to-sequence model such as T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d873f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: summarize: The quick brown fox jumps over the lazy dog. This sentence is often used to demonstrate fonts and test typewriters.\n",
      "Generated: sentence often used to demonstrate fonts and test typewriters .\n"
     ]
    }
   ],
   "source": [
    "# Text generation example with T5\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text2text-generation', model='t5-small')\n",
    "prompt = \"summarize: The quick brown fox jumps over the lazy dog. This sentence is often used to demonstrate fonts and test typewriters.\"\n",
    "generated = generator(prompt, max_length=40)\n",
    "print(f\"Prompt: {prompt}\\nGenerated: {generated[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b6843",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning a Model with PEFT/LoRA (Overview & Example)\n",
    "\n",
    "Fine-tuning allows you to adapt a pre-trained model to your own data. PEFT (Parameter-Efficient Fine-Tuning) and LoRA (Low-Rank Adaptation) are modern techniques for efficient fine-tuning.\n",
    "\n",
    "- **PEFT**: Only a small subset of parameters are updated, reducing compute and memory needs.\n",
    "- **LoRA**: Injects trainable low-rank matrices into each layer, making fine-tuning lightweight.\n",
    "\n",
    "Below is a minimal example using the `peft` library (conceptual, not runnable as-is):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0795729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4936eaf76f49c0b81b35e29652ff21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca09ddb167a4a27b7a598a5e54f3464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Minimal PEFT/LoRA fine-tuning example (conceptual)\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "lora_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "# Now peft_model can be trained as usual with your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fa549",
   "metadata": {},
   "source": [
    "## 9. Limitations of Pre-trained NLP Models\n",
    "\n",
    "While pre-trained models are powerful, they have important limitations:\n",
    "\n",
    "- **Context window size**: Most models have a maximum input length (e.g., 512 or 1024 tokens). Longer texts are truncated or split.\n",
    "- **Domain-specific vocabulary**: Performance drops on specialized jargon or rare terms not seen during pre-training.\n",
    "- **Bias and fairness**: Models may reflect biases present in their training data.\n",
    "- **Need for fine-tuning**: For best results on your data, fine-tuning is often required.\n",
    "\n",
    "**Example:**\n",
    "- Try classifying a very long text or a text with medical/legal jargon and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9eb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1002 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text classification:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1002) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m specialized_text = \u001b[33m\"\u001b[39m\u001b[33mThe patient was administered 5mg of apixaban for atrial fibrillation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLong text classification:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMedical jargon classification:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(classifier(specialized_text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:163\u001b[39m, in \u001b[36mTextClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    130\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m \u001b[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m inputs = (inputs,)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[32m    165\u001b[39m _legacy = \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/pipelines/base.py:1464\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1457\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1458\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1461\u001b[39m         )\n\u001b[32m   1462\u001b[39m     )\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/pipelines/base.py:1471\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1470\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/pipelines/base.py:1371\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1369\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1370\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:194\u001b[39m, in \u001b[36mTextClassificationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters.keys():\n\u001b[32m    193\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:917\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    910\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    912\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    913\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    915\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    927\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:723\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    721\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_flash_attention_2:\n\u001b[32m    726\u001b[39m     attention_mask = attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echobot-Fhr7ANK2-py3.13/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:126\u001b[39m, in \u001b[36mEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, input_embeds)\u001b[39m\n\u001b[32m    122\u001b[39m     position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[32m    124\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.position_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m embeddings = \u001b[43minput_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    127\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.LayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    128\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.dropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1002) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Limitation demo: context window and domain vocabulary\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "long_text = \" \".join([\"This is a sentence.\"] * 200)  # Exceeds most model limits\n",
    "specialized_text = \"The patient was administered 5mg of apixaban for atrial fibrillation.\"\n",
    "print(\"Long text classification:\")\n",
    "print(classifier(long_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528e0909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Medical jargon classification:\n",
      "[{'label': 'NEGATIVE', 'score': 0.9799715876579285}]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMedical jargon classification:\")\n",
    "print(classifier(specialized_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youarebot-quickstart",
   "language": "python",
   "name": "youarebot-quickstart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
